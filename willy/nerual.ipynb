{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path=\"mvc.csv\", sample_fraction=1.0):\n",
    "    # Read the CSV file\n",
    "    print(\"Loading full dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Take 1% random sample\n",
    "    print(f\"Taking {sample_fraction*100}% random sample...\")\n",
    "    df = df.sample(frac=sample_fraction, random_state=42)\n",
    "    print(f\"Sample size: {len(df)} records\")\n",
    "\n",
    "    # Extract time features\n",
    "    df[\"Hour\"] = df[\"Time\"].apply(\n",
    "        lambda x: int(x.split(\":\")[0]) if isinstance(x, str) else 0\n",
    "    )\n",
    "    df[\"Minute\"] = df[\"Time\"].apply(\n",
    "        lambda x: int(x.split(\":\")[1]) if isinstance(x, str) else 0\n",
    "    )\n",
    "\n",
    "    # Define features based on your column names\n",
    "    categorical_columns = [\n",
    "        \"Day of Week\",\n",
    "        \"Lighting Conditions\",\n",
    "        \"Municipality\",\n",
    "        \"Collision Type Descriptor\",\n",
    "        \"County Name\",\n",
    "        \"Road Descriptor\",\n",
    "        \"Weather Conditions\",\n",
    "        \"Traffic Control Device\",\n",
    "        \"Road Surface Conditions\",\n",
    "        \"Pedestrian Bicyclist Action\",\n",
    "        \"Event Descriptor\",\n",
    "    ]\n",
    "\n",
    "    binary_columns = [\"Police Report\"]\n",
    "    numeric_columns = [\"Hour\", \"Minute\", \"Number of Vehicles Involved\", \"Year\"]\n",
    "\n",
    "    # Process categorical columns\n",
    "    label_encoders = {}\n",
    "    for column in categorical_columns:\n",
    "        if column in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column].fillna(\"Unknown\"))\n",
    "            label_encoders[column] = le\n",
    "\n",
    "    # Process binary columns\n",
    "    for column in binary_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = (df[column] == \"Y\").astype(int)\n",
    "\n",
    "    # Create target variable from Crash Descriptor\n",
    "    def get_severity(crash_type):\n",
    "        if pd.isna(crash_type):\n",
    "            return 0  # Default to property damage\n",
    "        crash_type = str(crash_type).lower()\n",
    "        if \"fatal\" in crash_type:\n",
    "            return 2  # Fatal\n",
    "        elif \"injury\" in crash_type:\n",
    "            return 1  # Any Injury\n",
    "        else:\n",
    "            return 0  # Property Damage Only\n",
    "\n",
    "    df[\"Severity\"] = df[\"Crash Descriptor\"].apply(get_severity)\n",
    "\n",
    "    # Print class distribution\n",
    "    print(\"\\nClass distribution in sample:\")\n",
    "    print(df[\"Severity\"].value_counts().sort_index())\n",
    "\n",
    "    # Combine all features\n",
    "    feature_columns = categorical_columns + binary_columns + numeric_columns\n",
    "\n",
    "    # Handle missing values\n",
    "    for column in feature_columns:\n",
    "        if column in df.columns:\n",
    "            if df[column].dtype in [\"int64\", \"float64\"]:\n",
    "                df[column] = df[column].fillna(df[column].mean())\n",
    "            else:\n",
    "                df[column] = df[column].fillna(\"Unknown\")\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df[feature_columns])\n",
    "    y = df[\"Severity\"].values\n",
    "\n",
    "    return X, y, feature_columns, scaler, label_encoders, df\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=30, device=\"cpu\"\n",
    "):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    training_history = {\"train_loss\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        # Store history\n",
    "        training_history[\"train_loss\"].append(train_loss / len(train_loader))\n",
    "        training_history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "        training_history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "                f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\"\n",
    "            )\n",
    "\n",
    "    return training_history\n",
    "\n",
    "\n",
    "def print_stats(y_hat, y_test, model_description=\"\"):\n",
    "    print(f\"Model: {model_description}\")\n",
    "    acc = np.mean(y_hat == y_test)\n",
    "    print(f'accuracy: {acc}')\n",
    "    print()\n",
    "\n",
    "    print('test data counts')\n",
    "    print(f'no injury: {sum(y_test == 0)}, injury: {sum(y_test == 1)}')\n",
    "    print('predicted data counts')\n",
    "    print(f'no injury: {sum(y_hat == 0)}, injury: {sum(y_hat == 1)}')\n",
    "    print()\n",
    "\n",
    "    correct_guesses = np.sum(y_hat == y_test)\n",
    "    false_negatives = np.sum((y_hat == 0) & (y_test == 1))\n",
    "    false_positives = np.sum((y_hat == 1) & (y_test == 0))\n",
    "    true_negatives = np.sum((y_hat == 0) & (y_test == 0))\n",
    "    true_positives = np.sum((y_hat == 1) & (y_test == 1))\n",
    "\n",
    "    # Calculate total number of samples\n",
    "    total_samples = len(y_test)\n",
    "\n",
    "    # Calculate percentages\n",
    "    correct_guesses_percent = (correct_guesses / total_samples) * 100\n",
    "    false_negatives_percent = (false_negatives / total_samples) * 100\n",
    "    false_positives_percent = (false_positives / total_samples) * 100\n",
    "    true_negatives_percent = (true_negatives / total_samples) * 100\n",
    "    true_positives_percent = (true_positives / total_samples) * 100\n",
    "\n",
    "    # out of all predicted injuries, how many are actually injuries\n",
    "    precision = true_positives / (true_positives + false_positives) * 100\n",
    "\n",
    "    # out of all actual injuries, how many did the model correcly predict\n",
    "    recall = true_positives / (true_positives + false_negatives) * 100\n",
    "\n",
    "    # print(f\"Correct guesses: {correct_guesses} ({correct_guesses_percent:.2f}%)\")\n",
    "    print(f\"False negatives: {false_negatives} ({false_negatives_percent:.2f}%)\")\n",
    "    print(f\"False positives: {false_positives} ({false_positives_percent:.2f}%)\")\n",
    "    print(f\"True negatives: {true_negatives} ({true_negatives_percent:.2f}%)\")\n",
    "    print(f\"True positives: {true_positives} ({true_positives_percent:.2f}%)\")\n",
    "\n",
    "    print(f\"Out of all predicted injuries, how many are actually injuries: {precision:.2f}%\")\n",
    "    print(f\"Out of all actual injuries, how many did the model correctly predict: {recall:.2f}%\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y, feature_columns, scaler, label_encoders, df = load_and_preprocess_data(\n",
    "        \"mvc.csv\", sample_fraction=1\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = AccidentDataset(X_train, y_train)\n",
    "    test_dataset = AccidentDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    model = NeuralNetwork(input_size=len(feature_columns))\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = torch.FloatTensor(1.0 / class_counts)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    history = train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "\n",
    "    return model, scaler, label_encoders, history\n",
    "\n",
    "\n",
    "def main2():\n",
    "    X, y, feature_columns, scaler, label_encoders, df = load_and_preprocess_data(\n",
    "        \"mvc.csv\", sample_fraction=1\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = AccidentDataset(X_train, y_train)\n",
    "    test_dataset = AccidentDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    model = NeuralNetwork(input_size=len(feature_columns))\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = torch.FloatTensor(1.0 / class_counts)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    history = train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    model.eval()\n",
    "    y_hat = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_hat.extend(predicted.cpu().numpy())\n",
    "\n",
    "    y_hat = np.array(y_hat)\n",
    "\n",
    "    # Print stats\n",
    "    print_stats(y_hat, y_test, model_description=\"Neural Network for MVC Severity\")\n",
    "\n",
    "    return model, scaler, label_encoders, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset...\n",
      "Taking 100% random sample...\n",
      "Sample size: 772145 records\n",
      "\n",
      "Class distribution in sample:\n",
      "Severity\n",
      "0    546185\n",
      "1    223846\n",
      "2      2114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.7853, Val Loss: 0.7816, Val Accuracy: 70.75%\n",
      "Epoch [10/30], Train Loss: 0.7768, Val Loss: 0.7786, Val Accuracy: 70.77%\n",
      "Epoch [15/30], Train Loss: 0.7695, Val Loss: 0.7829, Val Accuracy: 70.97%\n",
      "Epoch [20/30], Train Loss: 0.7645, Val Loss: 0.7843, Val Accuracy: 70.57%\n",
      "Epoch [25/30], Train Loss: 0.7609, Val Loss: 0.7857, Val Accuracy: 72.09%\n",
      "Epoch [30/30], Train Loss: 0.7653, Val Loss: 0.8080, Val Accuracy: 71.50%\n",
      "Model: Neural Network for MVC Severity\n",
      "accuracy: 0.7149721123793407\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 153362, injury: 69727\n",
      "\n",
      "False negatives: 25500 (11.01%)\n",
      "False positives: 31738 (13.70%)\n",
      "True negatives: 127650 (55.11%)\n",
      "True positives: 37758 (16.30%)\n",
      "Out of all predicted injuries, how many are actually injuries: 54.33%\n",
      "Out of all actual injuries, how many did the model correctly predict: 59.69%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(NeuralNetwork(\n",
       "   (network): Sequential(\n",
       "     (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.1, inplace=False)\n",
       "     (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (4): ReLU()\n",
       "     (5): Dropout(p=0.1, inplace=False)\n",
       "     (6): Linear(in_features=128, out_features=3, bias=True)\n",
       "   )\n",
       " ),\n",
       " StandardScaler(),\n",
       " {'Day of Week': LabelEncoder(),\n",
       "  'Lighting Conditions': LabelEncoder(),\n",
       "  'Municipality': LabelEncoder(),\n",
       "  'Collision Type Descriptor': LabelEncoder(),\n",
       "  'County Name': LabelEncoder(),\n",
       "  'Road Descriptor': LabelEncoder(),\n",
       "  'Weather Conditions': LabelEncoder(),\n",
       "  'Traffic Control Device': LabelEncoder(),\n",
       "  'Road Surface Conditions': LabelEncoder(),\n",
       "  'Pedestrian Bicyclist Action': LabelEncoder(),\n",
       "  'Event Descriptor': LabelEncoder()},\n",
       " {'train_loss': [0.8390905341667613,\n",
       "   0.807167321722315,\n",
       "   0.7947318506130048,\n",
       "   0.786649471755322,\n",
       "   0.7853027680033534,\n",
       "   0.7842959861601666,\n",
       "   0.7786158447458427,\n",
       "   0.7782826190000599,\n",
       "   0.7799690690614052,\n",
       "   0.7767957957320775,\n",
       "   0.7685746709028184,\n",
       "   0.7710401929655601,\n",
       "   0.7714081649306175,\n",
       "   0.7713176822816905,\n",
       "   0.7695409330095467,\n",
       "   0.7683763761225961,\n",
       "   0.7699344653019599,\n",
       "   0.769645103470652,\n",
       "   0.7651895703829467,\n",
       "   0.7645225058742645,\n",
       "   0.7695782612616661,\n",
       "   0.7659820077591052,\n",
       "   0.7650346149331637,\n",
       "   0.7659461327160023,\n",
       "   0.7609073579022028,\n",
       "   0.7659910096395024,\n",
       "   0.7645134619297559,\n",
       "   0.7718921633723531,\n",
       "   0.7758043664762286,\n",
       "   0.7652723517515014],\n",
       "  'val_loss': [0.8372190633531433,\n",
       "   0.7989355849956281,\n",
       "   0.7876074598803705,\n",
       "   0.8006901179805644,\n",
       "   0.781574329762663,\n",
       "   0.7874862173561892,\n",
       "   0.7834940166186891,\n",
       "   0.8127538128946368,\n",
       "   0.789483766150738,\n",
       "   0.7785795207058198,\n",
       "   0.7855017430174746,\n",
       "   0.7874347811904402,\n",
       "   0.7806356092348942,\n",
       "   0.7795482161377675,\n",
       "   0.7828841307215928,\n",
       "   0.7834478876373386,\n",
       "   0.7890835089578154,\n",
       "   0.7928918538748889,\n",
       "   0.7866251701931598,\n",
       "   0.784275805052802,\n",
       "   0.7802873401310891,\n",
       "   0.796741722762914,\n",
       "   0.7895711143454794,\n",
       "   0.7966992066998179,\n",
       "   0.7856874137334732,\n",
       "   0.7909946945820066,\n",
       "   0.8057969848589344,\n",
       "   0.867942163673389,\n",
       "   0.7854029932898052,\n",
       "   0.8080343857284079],\n",
       "  'val_accuracy': [72.98483880437223,\n",
       "   71.90214294348225,\n",
       "   71.97639481272988,\n",
       "   72.89072887706999,\n",
       "   70.75080727322961,\n",
       "   69.84122187494604,\n",
       "   71.82012052977845,\n",
       "   74.21560670684326,\n",
       "   71.80242095629501,\n",
       "   70.77196042202691,\n",
       "   74.04163285040839,\n",
       "   71.6776605480824,\n",
       "   71.32280568458496,\n",
       "   72.04676140974944,\n",
       "   70.96665573034484,\n",
       "   73.27709761530625,\n",
       "   68.39374212153132,\n",
       "   71.08494068484399,\n",
       "   72.09208958574364,\n",
       "   70.56819947851012,\n",
       "   71.60945243563398,\n",
       "   71.50239160090484,\n",
       "   71.82271071126384,\n",
       "   70.7887966016819,\n",
       "   72.08906770734403,\n",
       "   70.90535476852412,\n",
       "   71.65305382397126,\n",
       "   70.85873150178722,\n",
       "   71.74673205435927,\n",
       "   71.49721123793407]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
