{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path=\"mvc.csv\", sample_fraction=1.0):\n",
    "    # Read the CSV file\n",
    "    print(\"Loading full dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Take 1% random sample\n",
    "    print(f\"Taking {sample_fraction*100}% random sample...\")\n",
    "    df = df.sample(frac=sample_fraction, random_state=42)\n",
    "    print(f\"Sample size: {len(df)} records\")\n",
    "\n",
    "    # Extract time features\n",
    "    df[\"Hour\"] = df[\"Time\"].apply(\n",
    "        lambda x: int(x.split(\":\")[0]) if isinstance(x, str) else 0\n",
    "    )\n",
    "    df[\"Minute\"] = df[\"Time\"].apply(\n",
    "        lambda x: int(x.split(\":\")[1]) if isinstance(x, str) else 0\n",
    "    )\n",
    "\n",
    "    # Define features based on your column names\n",
    "    categorical_columns = [\n",
    "        \"Day of Week\",\n",
    "        \"Lighting Conditions\",\n",
    "        \"Municipality\",\n",
    "        \"Collision Type Descriptor\",\n",
    "        \"County Name\",\n",
    "        \"Road Descriptor\",\n",
    "        \"Weather Conditions\",\n",
    "        \"Traffic Control Device\",\n",
    "        \"Road Surface Conditions\",\n",
    "        \"Pedestrian Bicyclist Action\",\n",
    "        \"Event Descriptor\",\n",
    "    ]\n",
    "\n",
    "    binary_columns = [\"Police Report\"]\n",
    "    numeric_columns = [\"Hour\", \"Minute\", \"Number of Vehicles Involved\", \"Year\"]\n",
    "\n",
    "    # Process categorical columns\n",
    "    label_encoders = {}\n",
    "    for column in categorical_columns:\n",
    "        if column in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column].fillna(\"Unknown\"))\n",
    "            label_encoders[column] = le\n",
    "\n",
    "    # Process binary columns\n",
    "    for column in binary_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = (df[column] == \"Y\").astype(int)\n",
    "\n",
    "    # Create target variable from Crash Descriptor\n",
    "    def get_severity(crash_type):\n",
    "        if pd.isna(crash_type):\n",
    "            return 0  # Default to property damage\n",
    "        crash_type = str(crash_type).lower()\n",
    "        if \"fatal\" in crash_type:\n",
    "            return 2  # Fatal\n",
    "        elif \"injury\" in crash_type:\n",
    "            return 1  # Any Injury\n",
    "        else:\n",
    "            return 0  # Property Damage Only\n",
    "\n",
    "    df[\"Severity\"] = df[\"Crash Descriptor\"].apply(get_severity)\n",
    "\n",
    "    # Print class distribution\n",
    "    print(\"\\nClass distribution in sample:\")\n",
    "    print(df[\"Severity\"].value_counts().sort_index())\n",
    "\n",
    "    # Combine all features\n",
    "    feature_columns = categorical_columns + binary_columns + numeric_columns\n",
    "\n",
    "    # Handle missing values\n",
    "    for column in feature_columns:\n",
    "        if column in df.columns:\n",
    "            if df[column].dtype in [\"int64\", \"float64\"]:\n",
    "                df[column] = df[column].fillna(df[column].mean())\n",
    "            else:\n",
    "                df[column] = df[column].fillna(\"Unknown\")\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df[feature_columns])\n",
    "    y = df[\"Severity\"].values\n",
    "\n",
    "    return X, y, feature_columns, scaler, label_encoders, df\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=30, device=\"cpu\"\n",
    "):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    training_history = {\"train_loss\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        # Store history\n",
    "        training_history[\"train_loss\"].append(train_loss / len(train_loader))\n",
    "        training_history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "        training_history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "                f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\"\n",
    "            )\n",
    "\n",
    "    return training_history\n",
    "\n",
    "\n",
    "def print_stats(y_hat, y_test, model_description=\"\"):\n",
    "    print(f\"Model: {model_description}\")\n",
    "    acc = np.mean(y_hat == y_test)\n",
    "    print(f'accuracy: {acc}')\n",
    "    print()\n",
    "\n",
    "    print('test data counts')\n",
    "    print(f'no injury: {sum(y_test == 0)}, injury: {sum(y_test == 1)}')\n",
    "    print('predicted data counts')\n",
    "    print(f'no injury: {sum(y_hat == 0)}, injury: {sum(y_hat == 1)}')\n",
    "    print()\n",
    "\n",
    "    correct_guesses = np.sum(y_hat == y_test)\n",
    "    false_negatives = np.sum((y_hat == 0) & (y_test == 1))\n",
    "    false_positives = np.sum((y_hat == 1) & (y_test == 0))\n",
    "    true_negatives = np.sum((y_hat == 0) & (y_test == 0))\n",
    "    true_positives = np.sum((y_hat == 1) & (y_test == 1))\n",
    "\n",
    "    # Calculate total number of samples\n",
    "    total_samples = len(y_test)\n",
    "\n",
    "    # Calculate percentages\n",
    "    correct_guesses_percent = (correct_guesses / total_samples) * 100\n",
    "    false_negatives_percent = (false_negatives / total_samples) * 100\n",
    "    false_positives_percent = (false_positives / total_samples) * 100\n",
    "    true_negatives_percent = (true_negatives / total_samples) * 100\n",
    "    true_positives_percent = (true_positives / total_samples) * 100\n",
    "\n",
    "    # out of all predicted injuries, how many are actually injuries\n",
    "    precision = true_positives / (true_positives + false_positives) * 100\n",
    "\n",
    "    # out of all actual injuries, how many did the model correcly predict\n",
    "    recall = true_positives / (true_positives + false_negatives) * 100\n",
    "\n",
    "    # print(f\"Correct guesses: {correct_guesses} ({correct_guesses_percent:.2f}%)\")\n",
    "    print(f\"False negatives: {false_negatives} ({false_negatives_percent:.2f}%)\")\n",
    "    print(f\"False positives: {false_positives} ({false_positives_percent:.2f}%)\")\n",
    "    print(f\"True negatives: {true_negatives} ({true_negatives_percent:.2f}%)\")\n",
    "    print(f\"True positives: {true_positives} ({true_positives_percent:.2f}%)\")\n",
    "\n",
    "    print(f\"Out of all predicted injuries, how many are actually injuries: {precision:.2f}%\")\n",
    "    print(f\"Out of all actual injuries, how many did the model correctly predict: {recall:.2f}%\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y, feature_columns, scaler, label_encoders, df = load_and_preprocess_data(\n",
    "        \"mvc.csv\", sample_fraction=1\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = AccidentDataset(X_train, y_train)\n",
    "    test_dataset = AccidentDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    model = NeuralNetwork(input_size=len(feature_columns))\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = torch.FloatTensor(1.0 / class_counts)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    history = train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "\n",
    "    return model, scaler, label_encoders, history\n",
    "\n",
    "\n",
    "def main2():\n",
    "    X, y, feature_columns, scaler, label_encoders, df = load_and_preprocess_data(\n",
    "        \"mvc.csv\", sample_fraction=1\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = AccidentDataset(X_train, y_train)\n",
    "    test_dataset = AccidentDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    model = NeuralNetwork(input_size=len(feature_columns))\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = torch.FloatTensor(1.0 / class_counts)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    history = train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "\n",
    "    model.eval()\n",
    "    y_hat = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_hat.extend(predicted.cpu().numpy())\n",
    "\n",
    "    y_hat = np.array(y_hat)\n",
    "\n",
    "    # Print stats\n",
    "    print_stats(y_hat, y_test, model_description=\"Neural Network for MVC Severity\")\n",
    "\n",
    "    return model, scaler, label_encoders, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset...\n",
      "Taking 100% random sample...\n",
      "Sample size: 772145 records\n",
      "\n",
      "Class distribution in sample:\n",
      "Severity\n",
      "0    546185\n",
      "1    223846\n",
      "2      2114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.7853, Val Loss: 0.7816, Val Accuracy: 70.75%\n",
      "Epoch [10/30], Train Loss: 0.7768, Val Loss: 0.7786, Val Accuracy: 70.77%\n",
      "Epoch [15/30], Train Loss: 0.7695, Val Loss: 0.7829, Val Accuracy: 70.97%\n",
      "Epoch [20/30], Train Loss: 0.7645, Val Loss: 0.7843, Val Accuracy: 70.57%\n",
      "Epoch [25/30], Train Loss: 0.7609, Val Loss: 0.7857, Val Accuracy: 72.09%\n",
      "Epoch [30/30], Train Loss: 0.7653, Val Loss: 0.8080, Val Accuracy: 71.50%\n",
      "Model: Neural Network for MVC Severity\n",
      "accuracy: 0.7149721123793407\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 153362, injury: 69727\n",
      "\n",
      "False negatives: 25500 (11.01%)\n",
      "False positives: 31738 (13.70%)\n",
      "True negatives: 127650 (55.11%)\n",
      "True positives: 37758 (16.30%)\n",
      "Out of all predicted injuries, how many are actually injuries: 54.33%\n",
      "Out of all actual injuries, how many did the model correctly predict: 59.69%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(NeuralNetwork(\n",
       "   (network): Sequential(\n",
       "     (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.1, inplace=False)\n",
       "     (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (4): ReLU()\n",
       "     (5): Dropout(p=0.1, inplace=False)\n",
       "     (6): Linear(in_features=128, out_features=3, bias=True)\n",
       "   )\n",
       " ),\n",
       " StandardScaler(),\n",
       " {'Day of Week': LabelEncoder(),\n",
       "  'Lighting Conditions': LabelEncoder(),\n",
       "  'Municipality': LabelEncoder(),\n",
       "  'Collision Type Descriptor': LabelEncoder(),\n",
       "  'County Name': LabelEncoder(),\n",
       "  'Road Descriptor': LabelEncoder(),\n",
       "  'Weather Conditions': LabelEncoder(),\n",
       "  'Traffic Control Device': LabelEncoder(),\n",
       "  'Road Surface Conditions': LabelEncoder(),\n",
       "  'Pedestrian Bicyclist Action': LabelEncoder(),\n",
       "  'Event Descriptor': LabelEncoder()},\n",
       " {'train_loss': [0.8390905341667613,\n",
       "   0.807167321722315,\n",
       "   0.7947318506130048,\n",
       "   0.786649471755322,\n",
       "   0.7853027680033534,\n",
       "   0.7842959861601666,\n",
       "   0.7786158447458427,\n",
       "   0.7782826190000599,\n",
       "   0.7799690690614052,\n",
       "   0.7767957957320775,\n",
       "   0.7685746709028184,\n",
       "   0.7710401929655601,\n",
       "   0.7714081649306175,\n",
       "   0.7713176822816905,\n",
       "   0.7695409330095467,\n",
       "   0.7683763761225961,\n",
       "   0.7699344653019599,\n",
       "   0.769645103470652,\n",
       "   0.7651895703829467,\n",
       "   0.7645225058742645,\n",
       "   0.7695782612616661,\n",
       "   0.7659820077591052,\n",
       "   0.7650346149331637,\n",
       "   0.7659461327160023,\n",
       "   0.7609073579022028,\n",
       "   0.7659910096395024,\n",
       "   0.7645134619297559,\n",
       "   0.7718921633723531,\n",
       "   0.7758043664762286,\n",
       "   0.7652723517515014],\n",
       "  'val_loss': [0.8372190633531433,\n",
       "   0.7989355849956281,\n",
       "   0.7876074598803705,\n",
       "   0.8006901179805644,\n",
       "   0.781574329762663,\n",
       "   0.7874862173561892,\n",
       "   0.7834940166186891,\n",
       "   0.8127538128946368,\n",
       "   0.789483766150738,\n",
       "   0.7785795207058198,\n",
       "   0.7855017430174746,\n",
       "   0.7874347811904402,\n",
       "   0.7806356092348942,\n",
       "   0.7795482161377675,\n",
       "   0.7828841307215928,\n",
       "   0.7834478876373386,\n",
       "   0.7890835089578154,\n",
       "   0.7928918538748889,\n",
       "   0.7866251701931598,\n",
       "   0.784275805052802,\n",
       "   0.7802873401310891,\n",
       "   0.796741722762914,\n",
       "   0.7895711143454794,\n",
       "   0.7966992066998179,\n",
       "   0.7856874137334732,\n",
       "   0.7909946945820066,\n",
       "   0.8057969848589344,\n",
       "   0.867942163673389,\n",
       "   0.7854029932898052,\n",
       "   0.8080343857284079],\n",
       "  'val_accuracy': [72.98483880437223,\n",
       "   71.90214294348225,\n",
       "   71.97639481272988,\n",
       "   72.89072887706999,\n",
       "   70.75080727322961,\n",
       "   69.84122187494604,\n",
       "   71.82012052977845,\n",
       "   74.21560670684326,\n",
       "   71.80242095629501,\n",
       "   70.77196042202691,\n",
       "   74.04163285040839,\n",
       "   71.6776605480824,\n",
       "   71.32280568458496,\n",
       "   72.04676140974944,\n",
       "   70.96665573034484,\n",
       "   73.27709761530625,\n",
       "   68.39374212153132,\n",
       "   71.08494068484399,\n",
       "   72.09208958574364,\n",
       "   70.56819947851012,\n",
       "   71.60945243563398,\n",
       "   71.50239160090484,\n",
       "   71.82271071126384,\n",
       "   70.7887966016819,\n",
       "   72.08906770734403,\n",
       "   70.90535476852412,\n",
       "   71.65305382397126,\n",
       "   70.85873150178722,\n",
       "   71.74673205435927,\n",
       "   71.49721123793407]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_size, hidden_layers, nodes_per_layer):\n",
    "    layers = []\n",
    "    in_features = input_size\n",
    "\n",
    "    # different hidden layers\n",
    "    for _ in range(hidden_layers):\n",
    "        layers.append(nn.Linear(in_features, nodes_per_layer))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.1))\n",
    "        in_features = nodes_per_layer\n",
    "\n",
    "    # output layer\n",
    "    layers.append(nn.Linear(nodes_per_layer, 3))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def main3(configurations):\n",
    "    X, y, feature_columns, scaler, label_encoders, df = load_and_preprocess_data(\n",
    "        \"mvc.csv\", sample_fraction=1\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = AccidentDataset(X_train, y_train)\n",
    "    test_dataset = AccidentDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    results = []\n",
    "    for config in configurations:\n",
    "        print(f\"\\nTesting configuration: {config}\")\n",
    "\n",
    "        # Create model\n",
    "        model = create_model(input_size=len(feature_columns),\n",
    "                             hidden_layers=config[\"hidden_layers\"],\n",
    "                             nodes_per_layer=config[\"nodes_per_layer\"])\n",
    "\n",
    "        class_counts = np.bincount(y_train)\n",
    "        class_weights = torch.FloatTensor(1.0 / class_counts)\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        history = train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "\n",
    "        model.eval()\n",
    "        y_hat = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, _ in test_loader:\n",
    "                outputs = model(batch_X)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                y_hat.extend(predicted.cpu().numpy())\n",
    "\n",
    "        y_hat = np.array(y_hat)\n",
    "\n",
    "        test_accuracy = np.mean(y_hat == y_test) * 100\n",
    "        results.append((config, test_accuracy))\n",
    "\n",
    "        print(f\"Test Accuracy for {config}: {test_accuracy:.2f}%\")\n",
    "        print_stats(y_hat, y_test, model_description=f\"Configuration {config}\")\n",
    "\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    for config, accuracy in results:\n",
    "        print(f\"Configuration {config}: Test Accuracy = {accuracy:.2f}%\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset...\n",
      "Taking 100% random sample...\n",
      "Sample size: 772145 records\n",
      "\n",
      "Class distribution in sample:\n",
      "Severity\n",
      "0    546185\n",
      "1    223846\n",
      "2      2114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing configuration: {'hidden_layers': 1, 'nodes_per_layer': 64}\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.8037, Val Loss: 0.8041, Val Accuracy: 70.24%\n",
      "Epoch [10/30], Train Loss: 0.7967, Val Loss: 0.8023, Val Accuracy: 68.50%\n",
      "Epoch [15/30], Train Loss: 0.7959, Val Loss: 0.8023, Val Accuracy: 69.62%\n",
      "Epoch [20/30], Train Loss: 0.7911, Val Loss: 0.8063, Val Accuracy: 70.07%\n",
      "Epoch [25/30], Train Loss: 0.7890, Val Loss: 0.7986, Val Accuracy: 69.36%\n",
      "Epoch [30/30], Train Loss: 0.7883, Val Loss: 0.7973, Val Accuracy: 72.21%\n",
      "Test Accuracy for {'hidden_layers': 1, 'nodes_per_layer': 64}: 72.21%\n",
      "Model: Configuration {'hidden_layers': 1, 'nodes_per_layer': 64}\n",
      "accuracy: 0.7221296472172817\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 163748, injury: 61863\n",
      "\n",
      "False negatives: 30285 (13.07%)\n",
      "False positives: 27767 (11.99%)\n",
      "True negatives: 133242 (57.52%)\n",
      "True positives: 33849 (14.61%)\n",
      "Out of all predicted injuries, how many are actually injuries: 54.94%\n",
      "Out of all actual injuries, how many did the model correctly predict: 52.78%\n",
      "\n",
      "Testing configuration: {'hidden_layers': 2, 'nodes_per_layer': 128}\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.7845, Val Loss: 0.7887, Val Accuracy: 70.65%\n",
      "Epoch [10/30], Train Loss: 0.7692, Val Loss: 0.7827, Val Accuracy: 70.14%\n",
      "Epoch [15/30], Train Loss: 0.7661, Val Loss: 0.8006, Val Accuracy: 70.76%\n",
      "Epoch [20/30], Train Loss: 0.7656, Val Loss: 0.7779, Val Accuracy: 72.49%\n",
      "Epoch [25/30], Train Loss: 0.7627, Val Loss: 0.7922, Val Accuracy: 72.75%\n",
      "Epoch [30/30], Train Loss: 0.7631, Val Loss: 0.8019, Val Accuracy: 71.40%\n",
      "Test Accuracy for {'hidden_layers': 2, 'nodes_per_layer': 128}: 71.40%\n",
      "Model: Configuration {'hidden_layers': 2, 'nodes_per_layer': 128}\n",
      "accuracy: 0.7140007943223222\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 150405, injury: 74983\n",
      "\n",
      "False negatives: 24510 (10.58%)\n",
      "False positives: 35176 (15.19%)\n",
      "True negatives: 125691 (54.26%)\n",
      "True positives: 39530 (17.06%)\n",
      "Out of all predicted injuries, how many are actually injuries: 52.91%\n",
      "Out of all actual injuries, how many did the model correctly predict: 61.73%\n",
      "\n",
      "Testing configuration: {'hidden_layers': 3, 'nodes_per_layer': 256}\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.8060, Val Loss: 0.8349, Val Accuracy: 71.66%\n",
      "Epoch [10/30], Train Loss: 0.8094, Val Loss: 0.7910, Val Accuracy: 70.87%\n",
      "Epoch [15/30], Train Loss: 0.8119, Val Loss: 0.8108, Val Accuracy: 74.09%\n",
      "Epoch [20/30], Train Loss: 0.8092, Val Loss: 0.8137, Val Accuracy: 71.89%\n",
      "Epoch [25/30], Train Loss: 0.8202, Val Loss: 0.8165, Val Accuracy: 70.90%\n",
      "Epoch [30/30], Train Loss: 0.8126, Val Loss: 0.8217, Val Accuracy: 72.26%\n",
      "Test Accuracy for {'hidden_layers': 3, 'nodes_per_layer': 256}: 72.26%\n",
      "Model: Configuration {'hidden_layers': 3, 'nodes_per_layer': 256}\n",
      "accuracy: 0.7226433665452159\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 155778, injury: 71422\n",
      "\n",
      "False negatives: 26419 (11.41%)\n",
      "False positives: 33075 (14.28%)\n",
      "True negatives: 129117 (55.74%)\n",
      "True positives: 38107 (16.45%)\n",
      "Out of all predicted injuries, how many are actually injuries: 53.53%\n",
      "Out of all actual injuries, how many did the model correctly predict: 59.06%\n",
      "\n",
      "Summary of Results:\n",
      "Configuration {'hidden_layers': 1, 'nodes_per_layer': 64}: Test Accuracy = 72.21%\n",
      "Configuration {'hidden_layers': 2, 'nodes_per_layer': 128}: Test Accuracy = 71.40%\n",
      "Configuration {'hidden_layers': 3, 'nodes_per_layer': 256}: Test Accuracy = 72.26%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'hidden_layers': 1, 'nodes_per_layer': 64}, np.float64(72.21296472172817)),\n",
       " ({'hidden_layers': 2, 'nodes_per_layer': 128}, np.float64(71.40007943223222)),\n",
       " ({'hidden_layers': 3, 'nodes_per_layer': 256}, np.float64(72.26433665452159))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurations = [\n",
    "    {\"hidden_layers\": 1, \"nodes_per_layer\": 64},\n",
    "    {\"hidden_layers\": 2, \"nodes_per_layer\": 128},\n",
    "    {\"hidden_layers\": 3, \"nodes_per_layer\": 256},\n",
    "]\n",
    "\n",
    "main3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset...\n",
      "Taking 100% random sample...\n",
      "Sample size: 772145 records\n",
      "\n",
      "Class distribution in sample:\n",
      "Severity\n",
      "0    546185\n",
      "1    223846\n",
      "2      2114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing configuration: {'hidden_layers': 1, 'nodes_per_layer': 512}\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.8006, Val Loss: 0.8018, Val Accuracy: 70.96%\n",
      "Epoch [10/30], Train Loss: 0.7911, Val Loss: 0.8253, Val Accuracy: 67.31%\n",
      "Epoch [15/30], Train Loss: 0.7854, Val Loss: 0.8071, Val Accuracy: 70.04%\n",
      "Epoch [20/30], Train Loss: 0.7802, Val Loss: 0.8164, Val Accuracy: 71.66%\n",
      "Epoch [25/30], Train Loss: 0.7791, Val Loss: 0.8031, Val Accuracy: 70.10%\n",
      "Epoch [30/30], Train Loss: 0.7738, Val Loss: 0.8055, Val Accuracy: 66.51%\n",
      "Test Accuracy for {'hidden_layers': 1, 'nodes_per_layer': 512}: 66.51%\n",
      "Model: Configuration {'hidden_layers': 1, 'nodes_per_layer': 512}\n",
      "accuracy: 0.6650636321251576\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 131924, injury: 89596\n",
      "\n",
      "False negatives: 20484 (8.84%)\n",
      "False positives: 46736 (20.18%)\n",
      "True negatives: 111297 (48.05%)\n",
      "True positives: 42555 (18.37%)\n",
      "Out of all predicted injuries, how many are actually injuries: 47.66%\n",
      "Out of all actual injuries, how many did the model correctly predict: 67.51%\n",
      "\n",
      "Testing configuration: {'hidden_layers': 2, 'nodes_per_layer': 256}\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.7887, Val Loss: 0.7857, Val Accuracy: 72.21%\n",
      "Epoch [10/30], Train Loss: 0.7868, Val Loss: 0.7942, Val Accuracy: 70.98%\n",
      "Epoch [15/30], Train Loss: 0.7786, Val Loss: 0.7936, Val Accuracy: 71.42%\n",
      "Epoch [20/30], Train Loss: 0.7931, Val Loss: 0.8214, Val Accuracy: 72.46%\n",
      "Epoch [25/30], Train Loss: 0.7878, Val Loss: 0.8193, Val Accuracy: 72.13%\n",
      "Epoch [30/30], Train Loss: 0.7993, Val Loss: 0.8141, Val Accuracy: 73.06%\n",
      "Test Accuracy for {'hidden_layers': 2, 'nodes_per_layer': 256}: 73.06%\n",
      "Model: Configuration {'hidden_layers': 2, 'nodes_per_layer': 256}\n",
      "accuracy: 0.730638393396764\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 160161, injury: 66143\n",
      "\n",
      "False negatives: 27684 (11.95%)\n",
      "False positives: 29034 (12.53%)\n",
      "True negatives: 132247 (57.09%)\n",
      "True positives: 36843 (15.91%)\n",
      "Out of all predicted injuries, how many are actually injuries: 55.93%\n",
      "Out of all actual injuries, how many did the model correctly predict: 57.10%\n",
      "\n",
      "Testing configuration: {'hidden_layers': 3, 'nodes_per_layer': 128}\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.8010, Val Loss: 0.8031, Val Accuracy: 69.15%\n",
      "Epoch [10/30], Train Loss: 0.7926, Val Loss: 0.7993, Val Accuracy: 67.96%\n",
      "Epoch [15/30], Train Loss: 0.7983, Val Loss: 0.8089, Val Accuracy: 69.90%\n",
      "Epoch [20/30], Train Loss: 0.7966, Val Loss: 0.8075, Val Accuracy: 70.15%\n",
      "Epoch [25/30], Train Loss: 0.7999, Val Loss: 0.7931, Val Accuracy: 72.60%\n",
      "Epoch [30/30], Train Loss: 0.7929, Val Loss: 0.8279, Val Accuracy: 69.33%\n",
      "Test Accuracy for {'hidden_layers': 3, 'nodes_per_layer': 128}: 69.33%\n",
      "Model: Configuration {'hidden_layers': 3, 'nodes_per_layer': 128}\n",
      "accuracy: 0.6932836594084025\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 142985, injury: 81241\n",
      "\n",
      "False negatives: 23194 (10.01%)\n",
      "False positives: 40151 (17.33%)\n",
      "True negatives: 119595 (51.63%)\n",
      "True positives: 40816 (17.62%)\n",
      "Out of all predicted injuries, how many are actually injuries: 50.41%\n",
      "Out of all actual injuries, how many did the model correctly predict: 63.77%\n",
      "\n",
      "Testing configuration: {'hidden_layers': 4, 'nodes_per_layer': 64}\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.7990, Val Loss: 0.8029, Val Accuracy: 69.45%\n",
      "Epoch [10/30], Train Loss: 0.7954, Val Loss: 0.7859, Val Accuracy: 70.61%\n",
      "Epoch [15/30], Train Loss: 0.7972, Val Loss: 0.7845, Val Accuracy: 71.69%\n",
      "Epoch [20/30], Train Loss: 0.7930, Val Loss: 0.7975, Val Accuracy: 72.02%\n",
      "Epoch [25/30], Train Loss: 0.7934, Val Loss: 0.7900, Val Accuracy: 71.64%\n",
      "Epoch [30/30], Train Loss: 0.7976, Val Loss: 0.7967, Val Accuracy: 70.23%\n",
      "Test Accuracy for {'hidden_layers': 4, 'nodes_per_layer': 64}: 70.23%\n",
      "Model: Configuration {'hidden_layers': 4, 'nodes_per_layer': 64}\n",
      "accuracy: 0.7023104418849614\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 152555, injury: 67806\n",
      "\n",
      "False negatives: 25753 (11.12%)\n",
      "False positives: 31736 (13.70%)\n",
      "True negatives: 126595 (54.65%)\n",
      "True positives: 35857 (15.48%)\n",
      "Out of all predicted injuries, how many are actually injuries: 53.05%\n",
      "Out of all actual injuries, how many did the model correctly predict: 58.20%\n",
      "\n",
      "Testing configuration: {'hidden_layers': 5, 'nodes_per_layer': 32}\n",
      "\n",
      "Starting training...\n",
      "Epoch [5/30], Train Loss: 0.8144, Val Loss: 0.8331, Val Accuracy: 73.05%\n",
      "Epoch [10/30], Train Loss: 0.8052, Val Loss: 0.7973, Val Accuracy: 70.06%\n",
      "Epoch [15/30], Train Loss: 0.8022, Val Loss: 0.7918, Val Accuracy: 67.43%\n",
      "Epoch [20/30], Train Loss: 0.7984, Val Loss: 0.7889, Val Accuracy: 69.10%\n",
      "Epoch [25/30], Train Loss: 0.7977, Val Loss: 0.7932, Val Accuracy: 71.47%\n",
      "Epoch [30/30], Train Loss: 0.7974, Val Loss: 0.7966, Val Accuracy: 72.69%\n",
      "Test Accuracy for {'hidden_layers': 5, 'nodes_per_layer': 32}: 72.69%\n",
      "Model: Configuration {'hidden_layers': 5, 'nodes_per_layer': 32}\n",
      "accuracy: 0.7268826302429591\n",
      "\n",
      "test data counts\n",
      "no injury: 164099, injury: 66891\n",
      "predicted data counts\n",
      "no injury: 163741, injury: 60840\n",
      "\n",
      "False negatives: 29548 (12.76%)\n",
      "False positives: 26359 (11.38%)\n",
      "True negatives: 133969 (57.83%)\n",
      "True positives: 34230 (14.78%)\n",
      "Out of all predicted injuries, how many are actually injuries: 56.50%\n",
      "Out of all actual injuries, how many did the model correctly predict: 53.67%\n",
      "\n",
      "Summary of Results:\n",
      "Configuration {'hidden_layers': 1, 'nodes_per_layer': 512}: Test Accuracy = 66.51%\n",
      "Configuration {'hidden_layers': 2, 'nodes_per_layer': 256}: Test Accuracy = 73.06%\n",
      "Configuration {'hidden_layers': 3, 'nodes_per_layer': 128}: Test Accuracy = 69.33%\n",
      "Configuration {'hidden_layers': 4, 'nodes_per_layer': 64}: Test Accuracy = 70.23%\n",
      "Configuration {'hidden_layers': 5, 'nodes_per_layer': 32}: Test Accuracy = 72.69%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'hidden_layers': 1, 'nodes_per_layer': 512}, np.float64(66.50636321251577)),\n",
       " ({'hidden_layers': 2, 'nodes_per_layer': 256}, np.float64(73.06383933967639)),\n",
       " ({'hidden_layers': 3, 'nodes_per_layer': 128}, np.float64(69.32836594084026)),\n",
       " ({'hidden_layers': 4, 'nodes_per_layer': 64}, np.float64(70.23104418849614)),\n",
       " ({'hidden_layers': 5, 'nodes_per_layer': 32}, np.float64(72.6882630242959))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurations = [\n",
    "    {\"hidden_layers\": 1, \"nodes_per_layer\": 512},\n",
    "    {\"hidden_layers\": 2, \"nodes_per_layer\": 256},\n",
    "    {\"hidden_layers\": 3, \"nodes_per_layer\": 128},\n",
    "    {\"hidden_layers\": 4, \"nodes_per_layer\": 64},\n",
    "    {\"hidden_layers\": 5, \"nodes_per_layer\": 32},\n",
    "]\n",
    "\n",
    "main3(configurations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
